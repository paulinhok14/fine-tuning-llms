{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a372f32a",
   "metadata": {},
   "source": [
    "## Fine-tuning LLMs with HuggingFace, PEFT (LoRa/QLoRA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435aab8e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "24b8f677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install transformers==4.40.2, peft==0.10.0\\n\",\n",
    "# %pip install accelerate==1.7.0\\n\",\n",
    "# %pip install bitsandbytes==0.41.1 --prefer-binary --extra-index-url=https://jllllll.github.io/bitsandbytes-windows-webui\\n\",\n",
    "# %pip install torch==2.2.1+cu121 torchvision==0.17.1+cu121 torchaudio==2.2.1+cu121 --index-url https://download.pytorch.org/whl/cu121\\n\",\n",
    "# %pip install bitsandbytes-cuda117==0.26.0.post2\\n\",\n",
    "# %pip install -i https://pypi.org/simple/ bitsandbytes\\n\",\n",
    "# %pip install trl\\n\",\n",
    "# %pip install numpy\\n\",\n",
    "# %pip install safetensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc886cae",
   "metadata": {},
   "source": [
    "### 0 - Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f549fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFace\n",
    "import transformers, peft\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, AutoPeftModelForCausalLM, prepare_model_for_kbit_training, get_peft_model\n",
    "from transformers import (\n",
    "    pipeline,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "from transformers.trainer_callback import TrainerCallback\n",
    "from transformers.integrations import MLflowCallback\n",
    "from trl import SFTTrainer\n",
    "import bitsandbytes as bnb\n",
    "from bitsandbytes.optim import AdamW8bit\n",
    "\n",
    "# Models/MLOps\n",
    "from ollama import chat\n",
    "import torch\n",
    "# import mlflow\n",
    "# import mlflow.transformers\n",
    "\n",
    "# System\n",
    "from dotenv import load_dotenv\n",
    "import os, sys, subprocess\n",
    "import gc # Garbage collector\n",
    "\n",
    "# Extras\n",
    "import accelerate\n",
    "from importlib.metadata import version\n",
    "import warnings\n",
    "from tqdm import tqdm # Progress bar\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "# Model and dataset configuration\n",
    "# model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "model_name = 'TinyLlama/TinyLlama-1.1B-Chat-v1.0' # More suitable for my GPU\n",
    "train_dataset_path = \"../data/training_dataset.jsonl\"\n",
    "output_model_dir = '../models/TinyLlama-1.1b-Chat-FineTuned-v1.0'\n",
    "merged_model_dir = '../models/TinyLlama-1.1b-Chat-FineTuned-v1.0-merged'\n",
    "\n",
    "# Loading environment variables\n",
    "load_dotenv()\n",
    "HF_TOKEN = os.getenv('HUGGINGFACE_TOKEN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1625a4f7",
   "metadata": {},
   "source": [
    "Asserting that transformers and peft lib versions are compatible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "63aedb61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers version: 4.52.4\n",
      "peft version: 0.15.2\n",
      "bitsandbytes version: 0.46.0\n",
      "trl version: 0.18.2\n",
      "accelerate version: 1.8.1\n",
      "PyTorch version: 2.2.1+cu121. - Must be a version with GPU (CUDA) support, not CPU only.\n"
     ]
    }
   ],
   "source": [
    "print('transformers version:', transformers.__version__)\n",
    "print('peft version:', peft.__version__)\n",
    "print('bitsandbytes version:', version('bitsandbytes'))\n",
    "print('trl version:', version('trl'))\n",
    "print('accelerate version:', accelerate.__version__)\n",
    "print(f\"PyTorch version: {torch.__version__}. - Must be a version with GPU (CUDA) support, not CPU only.\")\n",
    "\n",
    "# Asserting versions\n",
    "# assert transformers.__version__ == '4.40.2', 'transformers version mismatch'\n",
    "# assert peft.__version__ == '0.10.0', 'peft version mismatch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a842ecb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== NVIDIA Driver Check ===\n",
      "Thu Jun 26 18:32:14 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 576.57                 Driver Version: 576.57         CUDA Version: 12.9     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce GTX 1050      WDDM  |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   43C    P8            N/A  / 5001W |    1627MiB /   4096MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A            3452      C   ...s\\Python\\Python312\\python.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def check_nvidia_smi():\n",
    "    try:\n",
    "        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
    "        return result.stdout\n",
    "    except FileNotFoundError:\n",
    "        return \"nvidia-smi not found. NVIDIA drivers may not be installed.\"\n",
    "\n",
    "print(\"=== NVIDIA Driver Check ===\")\n",
    "print(check_nvidia_smi())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5aac66ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory: 4.3 GB\n",
      "Max memory for model: {0: '3.7 GB'}\n"
     ]
    }
   ],
   "source": [
    "# Get GPU memory info\n",
    "gpu_memory = torch.cuda.get_device_properties(0).total_memory\n",
    "max_memory = {0: f\"{gpu_memory * 0.85 / 1e9:.1f} GB\"}  # Use 85% of GPU memory\n",
    "\n",
    "print(f\"GPU Memory: {gpu_memory / 1e9:.1f} GB\")\n",
    "print(f\"Max memory for model: {max_memory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96bc41f",
   "metadata": {},
   "source": [
    "CUDA Availability Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5e540a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "Current CUDA device: 0\n",
      "CUDA version: 12.1\n",
      "CUDA device: NVIDIA GeForce GTX 1050\n",
      "CUDA capability: (6, 1)\n"
     ]
    }
   ],
   "source": [
    "# Check CUDA availability\n",
    "use_gpu = torch.cuda.is_available()\n",
    "print('CUDA Available:', use_gpu)\n",
    "print('Current CUDA device:', torch.cuda.current_device() if use_gpu else 'No CUDA device')\n",
    "print('CUDA version:', torch.version.cuda if use_gpu else 'Not Available')\n",
    "\n",
    "if use_gpu:\n",
    "    print(f'CUDA device: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'CUDA capability:', torch.cuda.get_device_capability(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e393afd6",
   "metadata": {},
   "source": [
    "#### Releasing GPU Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e2788c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bytes collected:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "607"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "print('Bytes collected:')\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bf04b8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ea8b22",
   "metadata": {},
   "source": [
    "### 1- Loading Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5c467ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data for Fine-Tuning\n",
    "dataset = load_dataset(\"json\", data_files=train_dataset_path, split=\"train\")\n",
    "\n",
    "\n",
    "# Formatting the dataset for training\n",
    "def formatting(example):\n",
    "    text = f\"### Prompt:\\n{example['prompt']}\\n\\n### Response:\\n{example['response']}\"\n",
    "    return {\"text\": text}\n",
    "\n",
    "dataset = dataset.map(formatting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1410dd",
   "metadata": {},
   "source": [
    "### 1.1- Loading Tokenizer and transforming Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ea393c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Tokenizing formatted dataset\n",
    "def preprocess(examples):\n",
    "    # Tokenize the texts with padding and truncation\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=512,\n",
    "        return_tensors=None  # Return lists instead of tensors\n",
    "    )\n",
    "    \n",
    "    # Set up the labels for training\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    \n",
    "    return tokenized\n",
    "\n",
    "# Apply preprocessing to the entire dataset at once\n",
    "tokenized_dataset = dataset.map(\n",
    "    preprocess,\n",
    "    batched=True,\n",
    "    remove_columns=dataset.column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5eae44",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcf2a43",
   "metadata": {},
   "source": [
    "### 2- Loading and Configuring Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "15824c53",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      2\u001b[39m bnb_config = BitsAndBytesConfig(\n\u001b[32m      3\u001b[39m     load_in_8bit=\u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# Enable 8-bit quantization\u001b[39;00m\n\u001b[32m      4\u001b[39m     llm_int8_threshold=\u001b[32m6.0\u001b[39m,  \u001b[38;5;66;03m# Threshold for outlier detection\u001b[39;00m\n\u001b[32m      5\u001b[39m     llm_int8_has_fp16_weight=\u001b[38;5;28;01mFalse\u001b[39;00m,  \u001b[38;5;66;03m# Disabling fp16 for weights to avoid dtype mismatch\u001b[39;00m\n\u001b[32m      6\u001b[39m     llm_int8_enable_fp32_cpu_offload=\u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# Offload to CPU\u001b[39;00m\n\u001b[32m      7\u001b[39m )\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Load Model with quantization\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbnb_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Important for training\u001b[39;49;00m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Let accelerate handle device mapping\u001b[39;49;00m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use float32 as base dtype\u001b[39;49;00m\n\u001b[32m     17\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Paulo\\Documents\\repos\\fine-tuning-llms\\venv\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:571\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    569\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    570\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m571\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    572\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    573\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    574\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    575\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    576\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    577\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Paulo\\Documents\\repos\\fine-tuning-llms\\venv\\Lib\\site-packages\\transformers\\modeling_utils.py:309\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    307\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    308\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m309\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    311\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Paulo\\Documents\\repos\\fine-tuning-llms\\venv\\Lib\\site-packages\\transformers\\modeling_utils.py:4574\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4564\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   4565\u001b[39m         torch.set_default_dtype(dtype_orig)\n\u001b[32m   4567\u001b[39m     (\n\u001b[32m   4568\u001b[39m         model,\n\u001b[32m   4569\u001b[39m         missing_keys,\n\u001b[32m   4570\u001b[39m         unexpected_keys,\n\u001b[32m   4571\u001b[39m         mismatched_keys,\n\u001b[32m   4572\u001b[39m         offload_index,\n\u001b[32m   4573\u001b[39m         error_msgs,\n\u001b[32m-> \u001b[39m\u001b[32m4574\u001b[39m     ) = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4575\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4576\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4577\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4578\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4579\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4580\u001b[39m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4581\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4582\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4583\u001b[39m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4584\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4585\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4586\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4587\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4588\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4589\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4590\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4592\u001b[39m \u001b[38;5;66;03m# record tp degree the model sharded to\u001b[39;00m\n\u001b[32m   4593\u001b[39m model._tp_size = tp_size\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Paulo\\Documents\\repos\\fine-tuning-llms\\venv\\Lib\\site-packages\\transformers\\modeling_utils.py:5031\u001b[39m, in \u001b[36mPreTrainedModel._load_pretrained_model\u001b[39m\u001b[34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001b[39m\n\u001b[32m   5029\u001b[39m \u001b[38;5;66;03m# Skip it with fsdp on ranks other than 0\u001b[39;00m\n\u001b[32m   5030\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (is_fsdp_enabled() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_local_dist_rank_0() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_quantized):\n\u001b[32m-> \u001b[39m\u001b[32m5031\u001b[39m     disk_offload_index, cpu_offload_index = \u001b[43m_load_state_dict_into_meta_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5032\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5033\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5034\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshard_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5035\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5036\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreverse_key_renaming_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5037\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5038\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5039\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisk_offload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5040\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcpu_offload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcpu_offload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5041\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcpu_offload_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcpu_offload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5042\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5043\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_safetensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_offloaded_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5044\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5045\u001b[39m \u001b[43m        \u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5046\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5047\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5049\u001b[39m \u001b[38;5;66;03m# force memory release if loading multiple shards, to avoid having 2 state dicts in memory in next loop\u001b[39;00m\n\u001b[32m   5050\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m state_dict\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Paulo\\Documents\\repos\\fine-tuning-llms\\venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Paulo\\Documents\\repos\\fine-tuning-llms\\venv\\Lib\\site-packages\\transformers\\modeling_utils.py:846\u001b[39m, in \u001b[36m_load_state_dict_into_meta_model\u001b[39m\u001b[34m(model, state_dict, shard_file, expected_keys, reverse_renaming_mapping, device_map, disk_offload_folder, disk_offload_index, cpu_offload_folder, cpu_offload_index, hf_quantizer, is_safetensors, keep_in_fp32_regex, unexpected_keys, device_mesh)\u001b[39m\n\u001b[32m    843\u001b[39m     _load_parameter_into_model(model, param_name, param.to(param_device))\n\u001b[32m    845\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m846\u001b[39m     \u001b[43mhf_quantizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_quantized_param\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    847\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munexpected_keys\u001b[49m\n\u001b[32m    848\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    849\u001b[39m     \u001b[38;5;66;03m# For quantized modules with FSDP/DeepSpeed Stage 3, we need to quantize the parameter on the GPU\u001b[39;00m\n\u001b[32m    850\u001b[39m     \u001b[38;5;66;03m# and then cast it to CPU to avoid excessive memory usage on each GPU\u001b[39;00m\n\u001b[32m    851\u001b[39m     \u001b[38;5;66;03m# in comparison to the sharded model across GPUs.\u001b[39;00m\n\u001b[32m    852\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_fsdp_enabled() \u001b[38;5;129;01mor\u001b[39;00m is_deepspeed_zero3_enabled():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Paulo\\Documents\\repos\\fine-tuning-llms\\venv\\Lib\\site-packages\\transformers\\quantizers\\quantizer_bnb_8bit.py:212\u001b[39m, in \u001b[36mBnb8BitHfQuantizer.create_quantized_param\u001b[39m\u001b[34m(self, model, param_value, param_name, target_device, state_dict, unexpected_keys)\u001b[39m\n\u001b[32m    205\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    206\u001b[39m     old_value.device == torch.device(\u001b[33m\"\u001b[39m\u001b[33mmeta\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    207\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m target_device \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mmeta\u001b[39m\u001b[33m\"\u001b[39m, torch.device(\u001b[33m\"\u001b[39m\u001b[33mmeta\u001b[39m\u001b[33m\"\u001b[39m)]\n\u001b[32m    208\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m param_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    209\u001b[39m ):\n\u001b[32m    210\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtensor_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is on the meta device, we need a `value` to put in on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_device\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m212\u001b[39m new_value = \u001b[43mparam_value\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcpu\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pre_quantized \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_serializable():\n\u001b[32m    214\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    215\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mDetected int8 weights but the version of bitsandbytes is not compatible with int8 serialization. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    216\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mMake sure to download the latest `bitsandbytes` version. `pip install --upgrade bitsandbytes`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    217\u001b[39m     )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Configure 8-bit quantization for efficient GPU usage\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,  # Enable 8-bit quantization\n",
    "    llm_int8_threshold=6.0,  # Threshold for outlier detection\n",
    "    llm_int8_has_fp16_weight=False,  # Disabling fp16 for weights to avoid dtype mismatch\n",
    "    llm_int8_enable_fp32_cpu_offload=True  # Offload to CPU\n",
    ")\n",
    "\n",
    "# Load Model with quantization\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=True,\n",
    "    use_cache=False,  # Important for training\n",
    "    device_map=\"auto\",  # Let accelerate handle device mapping\n",
    "    torch_dtype=torch.float32  # Use float32 as base dtype\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6785b3",
   "metadata": {},
   "source": [
    "### 2.1- Testing Model With Some Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dc17c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q: What is EEC?\n",
      "A: EEC is an acronym for Electrical Engineering Colleges. It refers to the set of institutions that offer undergraduate and graduate programs in electrical engineering, electrical engineering technology, and related fields.\n",
      "---\n",
      "\n",
      "Q: Explain what is APU.\n",
      "A: APU stands for Accelerated Pathways Unified. It is a program introduced by the California Community Colleges to provide students with a personalized learning experience that includes a blend of online and in-person learning. The program offers students the opportunity to earn two associate degrees within four years, while also completing the requirements for a transfer to a four-year university. The online classes are supplemented by in-person instruction, which helps to ensure that students have access to the resources and support they need to succeed.\n",
      "---\n",
      "\n",
      "Q: What can I see in MM03?\n",
      "A: In MM03, you can see the following:\n",
      "\n",
      "- A map of the world\n",
      "- A time-traveling camera\n",
      "- A world map with different time periods\n",
      "- Interactive maps and timelines\n",
      "- An interactive timeline that shows various events and milestones throughout history\n",
      "- A time-traveling puzzle board that allows you to solve puzzles and unlock hidden objects\n",
      "- An interactive time-traveling gallery of art and artifacts\n",
      "- A time-traveling art museum with a virtual 360-degree view of each exhibit\n",
      "- A virtual museum tour that takes you through different time periods\n",
      "- An interactive time-traveling map of the world that shows you the locations of historical events and sites\n",
      "- An interactive time-traveling storytelling tool that allows you to tell your own story and watch your progress appear on the world map\n",
      "---\n",
      "\n",
      "Q: What is AHEAD?\n",
      "A: AHEAD is an acronym for \"Advanced Higher Education in the UK.\" It is a popular academic term for higher education in the United Kingdom. It refers to both undergraduate and graduate programs that are designed to prepare students for careers in professional and academic fields.\n",
      "---\n",
      "\n",
      "Q: What is EPEP?\n",
      "A: EPEP is an acronym that stands for \"Electroencephalographic Perceptual Evaluation of Picture Quality.\"\n",
      "---\n",
      "\n",
      "Q: What is BER?\n",
      "A: BER is a program and dataset for machine learning tasks, such as computer vision and natural language processing, that comes with pre-trained layers and architectures that have been pre-trained on a large dataset. It is a widely-used dataset that is commonly used in these tasks.\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Test questions\n",
    "questions = [\n",
    "    \"What is EEC?\",\n",
    "    \"Explain what is APU.\",\n",
    "    \"What can I see in MM03?\",\n",
    "    \"What is AHEAD?\",\n",
    "    \"What is EPEP?\",\n",
    "    \"What is BER?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    prompt = f\"### Prompt:\\n{question}\\n\\n### Response:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(**inputs, \n",
    "                             max_new_tokens=256, \n",
    "                             do_sample=True,\n",
    "                             temperature=0.7)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True).split(\"### Response:\")[-1].strip()\n",
    "    print(f\"\\nQ: {question}\")\n",
    "    print(f\"A: {response}\\n---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5090fe",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a5c443",
   "metadata": {},
   "source": [
    "### 3- Preparing for PEFT - Applying LoRA/QLoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e33878d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare for PEFT\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# LoRA Config\n",
    "lora_config = LoraConfig(\n",
    "    r=8, # Rank of the LoRA matrix, the number of trainable parameters (the higher the more trainable parameters, but also more memory and computation)\n",
    "    lora_alpha=32, # Scaling factor for the LoRA matrix\n",
    "    target_modules=[\"q_proj\", \"v_proj\"], # Query and Value projection layers for TinyLlama\n",
    "    lora_dropout=0.05, # Dropout rate for the LoRA matrix\n",
    "    bias=\"none\", # Bias for the LoRA matrix (not used for TinyLlama)\n",
    "    task_type=\"CAUSAL_LM\" # Task type for the LoRA matrix\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59db8d3f",
   "metadata": {},
   "source": [
    "### Trainable Parameters Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b2d32f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,126,400 || all params: 1,101,174,784 || trainable%: 0.1023\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.print_trainable_parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4661d1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a61c77",
   "metadata": {},
   "source": [
    "### 4- Data Collator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910ab558",
   "metadata": {},
   "source": [
    "A data collator is a crucial component in the training pipeline that prepares batches of data for the model. \n",
    "In this case, we're using DataCollatorForLanguageModeling which:\n",
    "1. Pads sequences to the same length within each batch\n",
    "2. Creates attention masks to handle the padding\n",
    "3. Prepares the labels for language modeling\n",
    "\n",
    "The mlm=False parameter indicates we're doing causal language modeling (predicting next token) \n",
    "rather than masked language modeling (predicting masked tokens).\n",
    "\n",
    "This collator is necessary because:\n",
    "- It ensures all sequences in a batch have the same length through padding\n",
    "- It properly formats the input for the model's forward pass\n",
    "- It handles the creation of labels for the language modeling task\n",
    "- It optimizes memory usage by padding only within each batch rather than to a fixed length -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "424711c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer, \n",
    "                                                mlm=False) # As we are working with Causal Language Modeling (predicting next token), not MLM (Masked Language Modeling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d0d6df",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916420f4",
   "metadata": {},
   "source": [
    "### 5- Configure Training Arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20af8e32",
   "metadata": {},
   "source": [
    "I'll  avoid Trainer, its returning error when trying to train():\n",
    "\n",
    "\"The safest path on your setup is to avoid Trainer and instead train using a custom training loop with Accelerate, which gives you more control and avoids hidden offloading. Sometimes Hugging Face's Trainer tries to offload to CPU automatically if it detects low VRAM.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61adce46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "# Training Arguments - Optimized for GTX 1050 4GB VRAM with TinyLlama 1.1B + LoRA\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_model_dir,\n",
    "    per_device_train_batch_size=1,  # Keep small batch size for 4GB VRAM\n",
    "    gradient_accumulation_steps=16,  # Maintain effective batch size\n",
    "    optim=\"adamw_8bit\",  # Switching from Adamw_32bit to 8-bit optimizer from BitsAndBytes\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    logging_steps=20,\n",
    "    num_train_epochs=3,\n",
    "    max_steps=60, # Change to 200 when ready to production. I'll keep smaller for developing purposes\n",
    "    bf16=False,  # Disable bf16 since we're using fp16\n",
    "    fp16=True,  # Use fp16 for training\n",
    "    torch_compile=False,  # Disable torch compilation\n",
    "    gradient_checkpointing=True,\n",
    "    warmup_steps=50,\n",
    "    max_grad_norm=0.3,\n",
    "    ddp_find_unused_parameters=False,\n",
    "    dataloader_num_workers=0,       # For pre-loading batches in background\n",
    "    remove_unused_columns=False,\n",
    "    group_by_length=True,  # Group similar length sequences for efficiency\n",
    "    save_strategy=\"epoch\", \n",
    "    save_total_limit=5, # Only 5 last checkpoints\n",
    "    # report_to=\"mlflow\"  # MLFlow reporting\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    peft_config=lora_config,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[\n",
    "        # MLflowCallback()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e716bc3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4955b5",
   "metadata": {},
   "source": [
    "### 6 - Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9e805ed7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='160' max='160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [160/160 18:33, Epoch 160/160]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.249100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.058300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.046400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.043100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.042700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.041900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.041200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.041700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=160, training_loss=0.07055153772234916, metrics={'train_runtime': 1122.2936, 'train_samples_per_second': 2.281, 'train_steps_per_second': 0.143, 'total_flos': 163050970152960.0, 'train_loss': 0.07055153772234916})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Monitoring with MLFlow    \n",
    "# with mlflow.start_run():\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62ffc25",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc1dde8",
   "metadata": {},
   "source": [
    "### 7- Merging with Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8c397ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the peft_model that is already loaded in the trainer\n",
    "peft_model = trainer.model  # AutoPeftModelForCausalLM with the weights of the last checkpoint\n",
    "\n",
    "# Merge the LoRA delta-weights into the base model and remove the PEFT wrapper\n",
    "merged_model = peft_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c834022e",
   "metadata": {},
   "source": [
    "### 8- Saving Fine-Tuned LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07e5839",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'DTensor' from 'torch.distributed.tensor' (c:\\Users\\Paulo\\Documents\\repos\\fine-tuning-llms\\venv\\Lib\\site-packages\\torch\\distributed\\tensor\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Saving merged model\u001b[39;00m\n\u001b[32m      2\u001b[39m os.makedirs(merged_model_dir, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mmerged_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmerged_model_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Saving tokenizer\u001b[39;00m\n\u001b[32m      6\u001b[39m tokenizer.save_pretrained(merged_model_dir)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Paulo\\Documents\\repos\\fine-tuning-llms\\venv\\Lib\\site-packages\\transformers\\modeling_utils.py:3572\u001b[39m, in \u001b[36mPreTrainedModel.save_pretrained\u001b[39m\u001b[34m(self, save_directory, is_main_process, state_dict, save_function, push_to_hub, max_shard_size, safe_serialization, variant, token, save_peft_format, **kwargs)\u001b[39m\n\u001b[32m   3568\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, tensor \u001b[38;5;129;01min\u001b[39;00m state_dict.items():\n\u001b[32m   3569\u001b[39m     \u001b[38;5;66;03m# Sometimes in the state_dict we have non-tensor objects.\u001b[39;00m\n\u001b[32m   3570\u001b[39m     \u001b[38;5;66;03m# e.g. in bitsandbytes we have some `str` objects in the state_dict\u001b[39;00m\n\u001b[32m   3571\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensor, torch.Tensor) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensor, DTensor):\n\u001b[32m-> \u001b[39m\u001b[32m3572\u001b[39m         ptrs[\u001b[43mid_tensor_storage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m)\u001b[49m].append(name)\n\u001b[32m   3573\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3574\u001b[39m         \u001b[38;5;66;03m# In the non-tensor case, fall back to the pointer of the object itself\u001b[39;00m\n\u001b[32m   3575\u001b[39m         ptrs[\u001b[38;5;28mid\u001b[39m(tensor)].append(name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Paulo\\Documents\\repos\\fine-tuning-llms\\venv\\Lib\\site-packages\\transformers\\pytorch_utils.py:300\u001b[39m, in \u001b[36mid_tensor_storage\u001b[39m\u001b[34m(tensor)\u001b[39m\n\u001b[32m    293\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    294\u001b[39m \u001b[33;03mUnique identifier to a tensor storage. Multiple different tensors can share the same underlying storage. For\u001b[39;00m\n\u001b[32m    295\u001b[39m \u001b[33;03mexample, \"meta\" tensors all share the same storage, and thus their identifier will all be equal. This identifier is\u001b[39;00m\n\u001b[32m    296\u001b[39m \u001b[33;03mguaranteed to be unique and constant for this tensor's storage during its lifetime. Two tensor storages with\u001b[39;00m\n\u001b[32m    297\u001b[39m \u001b[33;03mnon-overlapping lifetimes may have the same id.\u001b[39;00m\n\u001b[32m    298\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    299\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_torch_greater_or_equal_than_2_0:\n\u001b[32m--> \u001b[39m\u001b[32m300\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistributed\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtensor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DTensor\n\u001b[32m    302\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensor, DTensor):\n\u001b[32m    303\u001b[39m         local_tensor = tensor.to_local()\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'DTensor' from 'torch.distributed.tensor' (c:\\Users\\Paulo\\Documents\\repos\\fine-tuning-llms\\venv\\Lib\\site-packages\\torch\\distributed\\tensor\\__init__.py)"
     ]
    }
   ],
   "source": [
    "# Saving merged model\n",
    "os.makedirs(merged_model_dir, exist_ok=True)\n",
    "merged_model.save_pretrained(merged_model_dir, safe_serialization=True)\n",
    "\n",
    "# Saving tokenizer\n",
    "tokenizer.save_pretrained(merged_model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd180df0",
   "metadata": {},
   "source": [
    "### 9- Testing Fine-Tuned Model in Same Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4bb904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assert model is in GPU\n",
    "merged_model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Making the same questions\n",
    "for question in questions:\n",
    "    prompt = f\"### Prompt:\\n{question}\\n\\n### Response:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(**inputs, \n",
    "                             max_new_tokens=256, \n",
    "                             do_sample=True,\n",
    "                             temperature=0.7)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True).split(\"### Response:\")[-1].strip()\n",
    "    print(f\"\\nQ: {question}\")\n",
    "    print(f\"A: {response}\\n---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
