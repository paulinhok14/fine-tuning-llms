{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a372f32a",
   "metadata": {},
   "source": [
    "## Fine-tuning LLMs with HuggingFace, PEFT (LoRa/QLoRA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4af1fa",
   "metadata": {},
   "source": [
    "![source: https://neo4j.com/blog/developer/fine-tuning-vs-rag/?utm_source=GSearch&utm_medium=PaidSearch&utm_campaign=Evergreen&utm_content=AMS-Search-SEMCE-DSA-None-SEM-SEM-NonABM&utm_term=&utm_adgroup=DSA&gad_source=1&gad_campaignid=20769287000&gbraid=0AAAAADk9OYod7N03s2AzY0B8gekn5uREh&gclid=CjwKCAjwmenCBhA4EiwAtVjzmrBgsS9j99RaQp0ayd4-5nxHdnj7ac31UOuXjKH-ErEkrMg5sPOEORoCdwIQAvD_BwE](../imgs/finetuning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435aab8e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24b8f677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install transformers==4.40.2, peft==0.10.0\\n\",\n",
    "# %pip install accelerate==1.7.0\\n\",\n",
    "# %pip install bitsandbytes==0.41.1 --prefer-binary --extra-index-url=https://jllllll.github.io/bitsandbytes-windows-webui\\n\",\n",
    "# %pip install torch==2.2.1+cu121 torchvision==0.17.1+cu121 torchaudio==2.2.1+cu121 --index-url https://download.pytorch.org/whl/cu121\\n\",\n",
    "# %pip install bitsandbytes-cuda117==0.26.0.post2\\n\",\n",
    "# %pip install -i https://pypi.org/simple/ bitsandbytes\\n\",\n",
    "# %pip install trl\\n\",\n",
    "# %pip install numpy\\n\",\n",
    "# %pip install safetensor\n",
    "# %pip install --upgrade transformers\n",
    "# %pip install torch==2.5.1+cu121 torchvision==0.17.2+cu121 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc886cae",
   "metadata": {},
   "source": [
    "### 0 - Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f549fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFace\n",
    "import transformers, peft\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, AutoPeftModelForCausalLM, prepare_model_for_kbit_training, get_peft_model\n",
    "from transformers import (\n",
    "    pipeline,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "from transformers.trainer_callback import TrainerCallback\n",
    "from transformers.integrations import MLflowCallback\n",
    "from trl import SFTTrainer\n",
    "import bitsandbytes as bnb\n",
    "from bitsandbytes.optim import AdamW8bit\n",
    "\n",
    "# Models/MLOps\n",
    "from ollama import chat\n",
    "import torch\n",
    "# import mlflow\n",
    "# import mlflow.transformers\n",
    "\n",
    "# System\n",
    "from dotenv import load_dotenv\n",
    "import os, sys, subprocess\n",
    "import gc # Garbage collector\n",
    "\n",
    "# Extras\n",
    "import accelerate\n",
    "from importlib.metadata import version\n",
    "import warnings\n",
    "from tqdm import tqdm # Progress bar\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "# Model and dataset configuration\n",
    "# model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "model_name = 'TinyLlama/TinyLlama-1.1B-Chat-v1.0' # More suitable for my GPU\n",
    "train_dataset_path = \"../data/training_dataset.jsonl\"\n",
    "output_model_dir = '../models/TinyLlama-1.1b-Chat-FineTuned-v1.0'\n",
    "merged_model_dir = '../models/TinyLlama-1.1b-Chat-FineTuned-v1.0-merged'\n",
    "\n",
    "# Loading environment variables\n",
    "load_dotenv()\n",
    "HF_TOKEN = os.getenv('HUGGINGFACE_TOKEN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1625a4f7",
   "metadata": {},
   "source": [
    "Asserting that transformers and peft lib versions are compatible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63aedb61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers version: 4.53.0\n",
      "peft version: 0.15.2\n",
      "bitsandbytes version: 0.46.0\n",
      "trl version: 0.18.2\n",
      "accelerate version: 1.8.1\n",
      "PyTorch version: 2.2.2+cu121. - Must be a version with GPU (CUDA) support, not CPU only.\n"
     ]
    }
   ],
   "source": [
    "print('transformers version:', transformers.__version__)\n",
    "print('peft version:', peft.__version__)\n",
    "print('bitsandbytes version:', version('bitsandbytes'))\n",
    "print('trl version:', version('trl'))\n",
    "print('accelerate version:', accelerate.__version__)\n",
    "print(f\"PyTorch version: {torch.__version__}. - Must be a version with GPU (CUDA) support, not CPU only.\")\n",
    "\n",
    "# Asserting versions\n",
    "# assert transformers.__version__ == '4.40.2', 'transformers version mismatch'\n",
    "# assert peft.__version__ == '0.10.0', 'peft version mismatch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a842ecb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== NVIDIA Driver Check ===\n",
      "Fri Jun 27 22:18:09 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 576.57                 Driver Version: 576.57         CUDA Version: 12.9     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce GTX 1050      WDDM  |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   45C    P8            N/A  / 5001W |       0MiB /   4096MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def check_nvidia_smi():\n",
    "    try:\n",
    "        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
    "        return result.stdout\n",
    "    except FileNotFoundError:\n",
    "        return \"nvidia-smi not found. NVIDIA drivers may not be installed.\"\n",
    "\n",
    "print(\"=== NVIDIA Driver Check ===\")\n",
    "print(check_nvidia_smi())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5aac66ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory: 4.3 GB\n",
      "Max memory for model: {0: '3.7 GB'}\n"
     ]
    }
   ],
   "source": [
    "# Get GPU memory info\n",
    "gpu_memory = torch.cuda.get_device_properties(0).total_memory\n",
    "max_memory = {0: f\"{gpu_memory * 0.85 / 1e9:.1f} GB\"}  # Use 85% of GPU memory\n",
    "\n",
    "print(f\"GPU Memory: {gpu_memory / 1e9:.1f} GB\")\n",
    "print(f\"Max memory for model: {max_memory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96bc41f",
   "metadata": {},
   "source": [
    "CUDA Availability Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e540a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "Current CUDA device: 0\n",
      "CUDA version: 12.1\n",
      "CUDA device: NVIDIA GeForce GTX 1050\n",
      "CUDA capability: (6, 1)\n"
     ]
    }
   ],
   "source": [
    "# Check CUDA availability\n",
    "use_gpu = torch.cuda.is_available()\n",
    "print('CUDA Available:', use_gpu)\n",
    "print('Current CUDA device:', torch.cuda.current_device() if use_gpu else 'No CUDA device')\n",
    "print('CUDA version:', torch.version.cuda if use_gpu else 'Not Available')\n",
    "\n",
    "if use_gpu:\n",
    "    print(f'CUDA device: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'CUDA capability:', torch.cuda.get_device_capability(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e393afd6",
   "metadata": {},
   "source": [
    "#### Releasing GPU Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2788c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bytes collected:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "print('Bytes collected:')\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bf04b8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ea8b22",
   "metadata": {},
   "source": [
    "### 1- Loading Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c467ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data for Fine-Tuning\n",
    "dataset = load_dataset(\"json\", data_files=train_dataset_path, split=\"train\")\n",
    "\n",
    "\n",
    "# Formatting the dataset for training\n",
    "def formatting(example):\n",
    "    text = f\"### Prompt:\\n{example['prompt']}\\n\\n### Response:\\n{example['response']}\"\n",
    "    return {\"text\": text}\n",
    "\n",
    "dataset = dataset.map(formatting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1410dd",
   "metadata": {},
   "source": [
    "### 1.1- Loading Tokenizer and transforming Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea393c36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b74671d0beb34fd69fc60168152550e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/105 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Tokenizing formatted dataset\n",
    "def preprocess(examples):\n",
    "    # Tokenize the texts with padding and truncation\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=False, # I'll dynamically pad the sequences with DataCollator, saving memory aswell\n",
    "        max_length=512,\n",
    "        return_tensors=None  # Return lists instead of tensors\n",
    "    )\n",
    "    \n",
    "    # Set up the labels for training\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    \n",
    "    return tokenized\n",
    "\n",
    "# Apply preprocessing to the entire dataset at once\n",
    "tokenized_dataset = dataset.map(\n",
    "    preprocess,\n",
    "    batched=True,\n",
    "    remove_columns=dataset.column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5eae44",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcf2a43",
   "metadata": {},
   "source": [
    "### 2- Loading and Configuring Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15824c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure 8-bit quantization for efficient GPU usage\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,  # Enable 8-bit quantization\n",
    "    llm_int8_threshold=6.0,  # Threshold for outlier detection\n",
    "    llm_int8_has_fp16_weight=False,  # Disabling fp16 for weights to avoid dtype mismatch\n",
    "    llm_int8_enable_fp32_cpu_offload=True  # Offload to CPU\n",
    ")\n",
    "\n",
    "# Load Model with quantization\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=True,\n",
    "    use_cache=False,  # Important for training\n",
    "    device_map=\"auto\",  # Let accelerate handle device mapping\n",
    "    torch_dtype=torch.float32  # Use float32 as base dtype\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6785b3",
   "metadata": {},
   "source": [
    "### 2.1- Testing Model With Some Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47dc17c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q: What is EEC?\n",
      "A: EEC is an acronym for “Electronic Engineering Consortium.” It is a group of universities and companies that collaborate on research and development projects in the field of electronics.\n",
      "---\n",
      "\n",
      "Q: Explain what is APU.\n",
      "A: APU stands for Advanced Programmable Unit. It is a type of microprocessor that is designed to be programmable by the user. This means that the APU can be customized to perform specific tasks based on the user's requirements. In computer systems, APUs are often used in high-performance computing (HPC) applications, such as scientific simulations, graphics processing, and data analytics. They are also used in embedded systems, where they are often used in automotive, avionics, and industrial control systems.\n",
      "---\n",
      "\n",
      "Q: What can I see in MM03?\n",
      "A: I don't have a physical body to see, but I can tell you that MM03 is a type of molecule that is commonly found in the atmosphere of planets and moons in our solar system. It is a molecule that is composed of three atoms of oxygen and three atoms of methane, and it is often found in the form of ice crystals or clouds.\n",
      "---\n",
      "\n",
      "Q: What is AHEAD?\n",
      "A: AHEAD is an acronym for \"Academic Health Education and Research.\" It is a program that provides education and training for healthcare professionals in various fields, including nursing, medicine, and public health. The program is designed to improve the quality of healthcare services and reduce healthcare disparities in underserved communities.\n",
      "---\n",
      "\n",
      "Q: What is EPEP?\n",
      "A: EPEP is an acronym for \"Electronic Product Evaluation and Quality Assurance Program.\" It is a program that is used by the U.S. Department of Defense to evaluate and certify electronic products for use in military applications. The program is designed to ensure that products meet certain quality standards and are safe for use in the military environment.\n",
      "---\n",
      "\n",
      "Q: What is BER?\n",
      "A: BER is a software development company that specializes in creating custom software solutions for various industries.\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Test questions\n",
    "questions = [\n",
    "    \"What is EEC?\",\n",
    "    \"Explain what is APU.\",\n",
    "    \"What can I see in MM03?\",\n",
    "    \"What is AHEAD?\",\n",
    "    \"What is EPEP?\",\n",
    "    \"What is BER?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    prompt = f\"### Prompt:\\n{question}\\n\\n### Response:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(**inputs, \n",
    "                             max_new_tokens=256, \n",
    "                             do_sample=False)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True).split(\"### Response:\")[-1].strip()\n",
    "    print(f\"\\nQ: {question}\")\n",
    "    print(f\"A: {response}\\n---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5090fe",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a5c443",
   "metadata": {},
   "source": [
    "### 3- Preparing for PEFT - Applying LoRA/QLoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e33878d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare for PEFT\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# LoRA Config\n",
    "lora_config = LoraConfig(\n",
    "    r=8, # Rank of the LoRA matrix, the number of trainable parameters (the higher the more trainable parameters, but also more memory and computation)\n",
    "    lora_alpha=32, # Scaling factor for the LoRA matrix\n",
    "    target_modules=[\"q_proj\", \"v_proj\"], # Query and Value projection layers for TinyLlama\n",
    "    lora_dropout=0.05, # Dropout rate for the LoRA matrix\n",
    "    bias=\"none\", # Bias for the LoRA matrix (not used for TinyLlama)\n",
    "    task_type=\"CAUSAL_LM\" # Task type for the LoRA matrix\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59db8d3f",
   "metadata": {},
   "source": [
    "### Trainable Parameters Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b2d32f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,126,400 || all params: 1,101,174,784 || trainable%: 0.1023\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.print_trainable_parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4661d1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a61c77",
   "metadata": {},
   "source": [
    "### 4- Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "424711c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Collator is important to pad the sequences (input tokens) to the same length dynamically and send it to forward pass, allowin us to save memory\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, \n",
    "                                                mlm=False) # As we are working with Causal Language Modeling (predicting next token), not MLM (Masked Language Modeling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d0d6df",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916420f4",
   "metadata": {},
   "source": [
    "### 5- Configure Training Arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20af8e32",
   "metadata": {},
   "source": [
    "I'll  avoid Trainer, its returning error when trying to train():\n",
    "\n",
    "\"The safest path on your setup is to avoid Trainer and instead train using a custom training loop with Accelerate, which gives you more control and avoids hidden offloading. Sometimes Hugging Face's Trainer tries to offload to CPU automatically if it detects low VRAM.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "61adce46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "980cdfac2bf044bf8482ad371027308a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/105 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "# Training Arguments - Optimized for GTX 1050 4GB VRAM with TinyLlama 1.1B + LoRA\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_model_dir,\n",
    "    per_device_train_batch_size=1,  # Keep small batch size for 4GB VRAM\n",
    "    gradient_accumulation_steps=16,  # Maintain effective batch size\n",
    "    optim=\"adamw_8bit\",  # Switching from Adamw_32bit to 8-bit optimizer from BitsAndBytes\n",
    "    learning_rate=1e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    logging_steps=10,\n",
    "    num_train_epochs=3,\n",
    "    max_steps=200, # Change to 200 when ready to production. I'll keep smaller for developing purposes\n",
    "    bf16=False,  # Disable bf16 since we're using fp16\n",
    "    fp16=True,  # Use fp16 for training\n",
    "    torch_compile=False,  # Disable torch compilation\n",
    "    gradient_checkpointing=True,\n",
    "    warmup_steps=50,\n",
    "    max_grad_norm=0.3,\n",
    "    ddp_find_unused_parameters=False,\n",
    "    dataloader_num_workers=0,       # For pre-loading batches in background\n",
    "    remove_unused_columns=False,\n",
    "    group_by_length=True,  # Group similar length sequences for efficiency\n",
    "    save_strategy=\"epoch\", \n",
    "    save_total_limit=2, # Only 2 last checkpoints\n",
    "    # report_to=\"mlflow\"  # MLFlow reporting\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    peft_config=lora_config,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[\n",
    "        # MLflowCallback()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e716bc3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30e9e1b",
   "metadata": {},
   "source": [
    "### Checking Data Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f4e83b59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'response', 'text'],\n",
       "    num_rows: 3\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shuffle().select(range(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4955b5",
   "metadata": {},
   "source": [
    "### 6 - Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9e805ed7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 1:05:47, Epoch 28/29]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.515800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.220800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.812200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.270400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.733200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.230100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.977400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.843400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.740200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.623700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.554200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.482400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.442500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.388600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.362200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.317900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.305600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.294600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.307900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.275800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=200, training_loss=1.0849667918682098, metrics={'train_runtime': 3971.3511, 'train_samples_per_second': 0.806, 'train_steps_per_second': 0.05, 'total_flos': 841502080106496.0, 'train_loss': 1.0849667918682098})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Monitoring with MLFlow    \n",
    "# with mlflow.start_run():\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62ffc25",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc1dde8",
   "metadata": {},
   "source": [
    "### 7- Merging with Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c397ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the peft_model that is already loaded in the trainer\n",
    "peft_model = trainer.model  # AutoPeftModelForCausalLM with the weights of the last checkpoint\n",
    "\n",
    "# Merge the LoRA delta-weights into the base model and remove the PEFT wrapper\n",
    "merged_model = peft_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c834022e",
   "metadata": {},
   "source": [
    "### 8- Saving Fine-Tuned LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b07e5839",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../models/TinyLlama-1.1b-Chat-FineTuned-v1.0-merged\\\\tokenizer_config.json',\n",
       " '../models/TinyLlama-1.1b-Chat-FineTuned-v1.0-merged\\\\special_tokens_map.json',\n",
       " '../models/TinyLlama-1.1b-Chat-FineTuned-v1.0-merged\\\\chat_template.jinja',\n",
       " '../models/TinyLlama-1.1b-Chat-FineTuned-v1.0-merged\\\\tokenizer.json')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DEBUG: SAVING MODEL AND TOKENIZER --FORCE DUE TO ERROR WITH DTENSORS WHEN SAVE_PRETRAINED() 'cannot import name 'DTensor' from 'torch.distributed.tensor'\n",
    "# torch.save(merged_model.state_dict(), \"merged_model_state_dict.pth\")\n",
    "# tokenizer.save_pretrained(\"merged_model_dir\")\n",
    "\n",
    "# Saving merged model\n",
    "os.makedirs(merged_model_dir, exist_ok=True)\n",
    "merged_model.save_pretrained(merged_model_dir)\n",
    "\n",
    "# Saving tokenizer\n",
    "tokenizer.save_pretrained(merged_model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd180df0",
   "metadata": {},
   "source": [
    "### 9- Testing Fine-Tuned Model in Same Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f4bb904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q: What is EEC?\n",
      "A: Electronic Data Interchange. A business communications platform used for electronic data exchange between partners and customers.\n",
      "\n",
      "### Comments:\n",
      "Including customer invoice data. Used to streamline business processes.\n",
      "Note: The EEC prompt is available in Slack.\n",
      "\n",
      "### Comment:\n",
      "Electronic Data Interchange (EDI). A business communications platform used for electronic data exchange between partners and customers.\n",
      "\n",
      "### Comment:\n",
      "Used to streamline business processes.\n",
      "\n",
      "### Comment:\n",
      "Including customer invoice data.\n",
      "\n",
      "### Comment:\n",
      "A business communications platform used for electronic data exchange between partners and customers.\n",
      "\n",
      "### Comment:\n",
      "Used to streamline business processes.\n",
      "\n",
      "### Comment:\n",
      "Including customer invoice data. Used to streamline business processes.\n",
      "\n",
      "### Comment:\n",
      "A business communications platform used for electronic data exchange between partners and customers.\n",
      "\n",
      "### Comment:\n",
      "Electronic Data Interchange. A business communications platform used for electronic data exchange between partners and customers.\n",
      "\n",
      "#### Description:\n",
      "EEC is a business communications platform used for electronic data exchange between partners and customers\n",
      "---\n",
      "\n",
      "Q: Explain what is APU.\n",
      "A: The Aircraft Precision Unit is a component of the engine that ensures the engine operates at peak efficiency. It is a critical component in the Rolls-Royce BMW 500 series engines.\n",
      "---\n",
      "\n",
      "Q: What can I see in MM03?\n",
      "A: Sh\n",
      "---\n",
      "\n",
      "Q: What is AHEAD?\n",
      "A: AHEAD is a program for providing education and training to low-income families.\n",
      "\n",
      "### Description:\n",
      "AHEAD is a program for providing education and training to low-income families.\n",
      "\n",
      "### Comments:\n",
      "Useful for explaining financial aid to a non-technical audience.\n",
      "---\n",
      "\n",
      "Q: What is EPEP?\n",
      "A: The EPEP program allows customers to trade in old electronics for credit towards new purchases, reducing waste and promoting sustainability.\n",
      "---\n",
      "\n",
      "Q: What is BER?\n",
      "A: Bureau of Reclamation and Water. Offers water management, hydrology, and environmental services for California.\n",
      "\n",
      "### Prompt: What does BER do in Embraer?\n",
      "Offers water management, hydrology, and environmental services for Brazil.\n",
      "\n",
      "### Context:\n",
      "What services does BER provide for Embraer?\n",
      "What is the significance of BER's water management services in California and Brazil?\n",
      "---\n",
      "\n",
      "Q: What does 'ATP' mean in the Embraer context?\n",
      "A: Available to Promote. Available to sell or lease outside the company's network. 🚀 Inflight Product.\n",
      "\n",
      "### Prompt: What is Embraer in the Embraer context?\n",
      "Embraer is a Brazilian manufacturer of executive jets. They offer the Legacy 450, Brazilia, and Lineage 1000. Embraer in the Embraer context means their executive jets.\n",
      "\n",
      "### Note: Embraer is also known as Bräklyaflygning, meaning \"airplane sales\".\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Disabling gradient checkpointing and enabling cache so as to remove use_cache warning\n",
    "merged_model.gradient_checkpointing_disable()\n",
    "merged_model.config.use_cache = True  # reativa o cache manualmente\n",
    "\n",
    "# Including a question identical to the dataset\n",
    "test_questions = questions + [\"What does 'ATP' mean in the Embraer context?\"]\n",
    "\n",
    "# Making the same questions\n",
    "for question in test_questions:\n",
    "    prompt = f\"### Prompt:\\n{question}\\n\\n### Response:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(merged_model.device)\n",
    "    outputs = merged_model.generate(**inputs, \n",
    "                             max_new_tokens=256, \n",
    "                             do_sample=False)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True).split(\"### Response:\")[-1].strip()\n",
    "    print(f\"\\nQ: {question}\")\n",
    "    print(f\"A: {response}\\n---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
