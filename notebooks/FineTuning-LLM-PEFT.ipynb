{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a372f32a",
   "metadata": {},
   "source": [
    "## Fine-tuning LLMs with HuggingFace, PEFT (LoRa/QLoRA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435aab8e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24b8f677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install transformers==4.40.2, peft==0.10.0\\n\",\n",
    "# %pip install accelerate==1.7.0\\n\",\n",
    "# %pip install bitsandbytes==0.41.1 --prefer-binary --extra-index-url=https://jllllll.github.io/bitsandbytes-windows-webui\\n\",\n",
    "# %pip install torch==2.2.1+cu121 torchvision==0.17.1+cu121 torchaudio==2.2.1+cu121 --index-url https://download.pytorch.org/whl/cu121\\n\",\n",
    "# %pip install bitsandbytes-cuda117==0.26.0.post2\\n\",\n",
    "# %pip install -i https://pypi.org/simple/ bitsandbytes\\n\",\n",
    "# %pip install trl\\n\",\n",
    "# %pip install numpy\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc886cae",
   "metadata": {},
   "source": [
    "### 0 - Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f549fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFace\n",
    "import transformers, peft\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, AutoPeftModelForCausalLM, prepare_model_for_kbit_training, get_peft_model\n",
    "from transformers import (\n",
    "    pipeline,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "from transformers.trainer_callback import TrainerCallback\n",
    "from transformers.integrations import MLflowCallback\n",
    "from trl import SFTTrainer\n",
    "import bitsandbytes as bnb\n",
    "from bitsandbytes.optim import AdamW8bit\n",
    "\n",
    "# Models/MLOps\n",
    "from ollama import chat\n",
    "import torch\n",
    "# import mlflow\n",
    "# import mlflow.transformers\n",
    "\n",
    "# System\n",
    "from dotenv import load_dotenv\n",
    "import os, sys, subprocess\n",
    "import gc # Garbage collector\n",
    "\n",
    "# Extras\n",
    "import accelerate\n",
    "from importlib.metadata import version\n",
    "import warnings\n",
    "from tqdm import tqdm # Progress bar\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "# Model and dataset configuration\n",
    "# model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "model_name = 'TinyLlama/TinyLlama-1.1B-Chat-v1.0' # More suitable for my GPU\n",
    "train_dataset_path = \"../data/training_dataset.jsonl\"\n",
    "output_model_dir = '../models/TinyLlama-1.1b-Chat-FineTuned-v1.0'\n",
    "merged_model_dir = '../models/TinyLlama-1.1b-Chat-FineTuned-v1.0-merged'\n",
    "\n",
    "# Loading environment variables\n",
    "load_dotenv()\n",
    "HF_TOKEN = os.getenv('HUGGINGFACE_TOKEN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1625a4f7",
   "metadata": {},
   "source": [
    "Asserting that transformers and peft lib versions are compatible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63aedb61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers version: 4.52.4\n",
      "peft version: 0.15.2\n",
      "bitsandbytes version: 0.46.0\n",
      "trl version: 0.18.2\n",
      "accelerate version: 1.8.1\n",
      "PyTorch version: 2.2.1+cu121. - Must be a version with GPU (CUDA) support, not CPU only.\n"
     ]
    }
   ],
   "source": [
    "print('transformers version:', transformers.__version__)\n",
    "print('peft version:', peft.__version__)\n",
    "print('bitsandbytes version:', version('bitsandbytes'))\n",
    "print('trl version:', version('trl'))\n",
    "print('accelerate version:', accelerate.__version__)\n",
    "print(f\"PyTorch version: {torch.__version__}. - Must be a version with GPU (CUDA) support, not CPU only.\")\n",
    "\n",
    "# Asserting versions\n",
    "# assert transformers.__version__ == '4.40.2', 'transformers version mismatch'\n",
    "# assert peft.__version__ == '0.10.0', 'peft version mismatch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a842ecb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== NVIDIA Driver Check ===\n",
      "Thu Jun 26 18:14:20 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 576.57                 Driver Version: 576.57         CUDA Version: 12.9     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce GTX 1050      WDDM  |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   45C    P8            N/A  / 5001W |       0MiB /   4096MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def check_nvidia_smi():\n",
    "    try:\n",
    "        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
    "        return result.stdout\n",
    "    except FileNotFoundError:\n",
    "        return \"nvidia-smi not found. NVIDIA drivers may not be installed.\"\n",
    "\n",
    "print(\"=== NVIDIA Driver Check ===\")\n",
    "print(check_nvidia_smi())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5aac66ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory: 4.3 GB\n",
      "Max memory for model: {0: '3.7 GB'}\n"
     ]
    }
   ],
   "source": [
    "# Get GPU memory info\n",
    "gpu_memory = torch.cuda.get_device_properties(0).total_memory\n",
    "max_memory = {0: f\"{gpu_memory * 0.85 / 1e9:.1f} GB\"}  # Use 85% of GPU memory\n",
    "\n",
    "print(f\"GPU Memory: {gpu_memory / 1e9:.1f} GB\")\n",
    "print(f\"Max memory for model: {max_memory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96bc41f",
   "metadata": {},
   "source": [
    "CUDA Availability Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e540a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "Current CUDA device: 0\n",
      "CUDA version: 12.1\n",
      "CUDA device: NVIDIA GeForce GTX 1050\n",
      "CUDA capability: (6, 1)\n"
     ]
    }
   ],
   "source": [
    "# Check CUDA availability\n",
    "use_gpu = torch.cuda.is_available()\n",
    "print('CUDA Available:', use_gpu)\n",
    "print('Current CUDA device:', torch.cuda.current_device() if use_gpu else 'No CUDA device')\n",
    "print('CUDA version:', torch.version.cuda if use_gpu else 'Not Available')\n",
    "\n",
    "if use_gpu:\n",
    "    print(f'CUDA device: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'CUDA capability:', torch.cuda.get_device_capability(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e393afd6",
   "metadata": {},
   "source": [
    "#### Releasing GPU Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2788c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bytes collected:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "print('Bytes collected:')\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bf04b8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ea8b22",
   "metadata": {},
   "source": [
    "### 1- Loading Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c467ee3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9b462422b21457ab3df285f437b381a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e55b6b90dec945b6b1a0cd8d7245abca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/105 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training data for Fine-Tuning\n",
    "dataset = load_dataset(\"json\", data_files=train_dataset_path, split=\"train\")\n",
    "\n",
    "\n",
    "# Formatting the dataset for training\n",
    "def formatting(example):\n",
    "    text = f\"### Prompt:\\n{example['prompt']}\\n\\n### Response:\\n{example['response']}\"\n",
    "    return {\"text\": text}\n",
    "\n",
    "dataset = dataset.map(formatting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1410dd",
   "metadata": {},
   "source": [
    "### 1.1- Loading Tokenizer and transforming Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea393c36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1276aceb652b425fb2c2d3fd17e9d2a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/105 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Tokenizing formatted dataset\n",
    "def preprocess(examples):\n",
    "    # Tokenize the texts with padding and truncation\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=512,\n",
    "        return_tensors=None  # Return lists instead of tensors\n",
    "    )\n",
    "    \n",
    "    # Set up the labels for training\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    \n",
    "    return tokenized\n",
    "\n",
    "# Apply preprocessing to the entire dataset at once\n",
    "tokenized_dataset = dataset.map(\n",
    "    preprocess,\n",
    "    batched=True,\n",
    "    remove_columns=dataset.column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5eae44",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcf2a43",
   "metadata": {},
   "source": [
    "### 2- Loading and Configuring Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15824c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure 8-bit quantization for efficient GPU usage\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,  # Enable 8-bit quantization\n",
    "    llm_int8_threshold=6.0,  # Threshold for outlier detection\n",
    "    llm_int8_has_fp16_weight=False,  # Disabling fp16 for weights to avoid dtype mismatch\n",
    "    llm_int8_enable_fp32_cpu_offload=True  # Offload to CPU\n",
    ")\n",
    "\n",
    "# Load Model with quantization\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=True,\n",
    "    use_cache=False,  # Important for training\n",
    "    device_map=\"auto\",  # Let accelerate handle device mapping\n",
    "    torch_dtype=torch.float32  # Use float32 as base dtype\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6785b3",
   "metadata": {},
   "source": [
    "### 2.1- Testing Model With Some Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47dc17c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q: What is EEC?\n",
      "A: EEC is an acronym for Electrical Engineering Colleges. It refers to the set of institutions that offer undergraduate and graduate programs in electrical engineering, electrical engineering technology, and related fields.\n",
      "---\n",
      "\n",
      "Q: Explain what is APU.\n",
      "A: APU stands for Accelerated Pathways Unified. It is a program introduced by the California Community Colleges to provide students with a personalized learning experience that includes a blend of online and in-person learning. The program offers students the opportunity to earn two associate degrees within four years, while also completing the requirements for a transfer to a four-year university. The online classes are supplemented by in-person instruction, which helps to ensure that students have access to the resources and support they need to succeed.\n",
      "---\n",
      "\n",
      "Q: What can I see in MM03?\n",
      "A: In MM03, you can see the following:\n",
      "\n",
      "- A map of the world\n",
      "- A time-traveling camera\n",
      "- A world map with different time periods\n",
      "- Interactive maps and timelines\n",
      "- An interactive timeline that shows various events and milestones throughout history\n",
      "- A time-traveling puzzle board that allows you to solve puzzles and unlock hidden objects\n",
      "- An interactive time-traveling gallery of art and artifacts\n",
      "- A time-traveling art museum with a virtual 360-degree view of each exhibit\n",
      "- A virtual museum tour that takes you through different time periods\n",
      "- An interactive time-traveling map of the world that shows you the locations of historical events and sites\n",
      "- An interactive time-traveling storytelling tool that allows you to tell your own story and watch your progress appear on the world map\n",
      "---\n",
      "\n",
      "Q: What is AHEAD?\n",
      "A: AHEAD is an acronym for \"Advanced Higher Education in the UK.\" It is a popular academic term for higher education in the United Kingdom. It refers to both undergraduate and graduate programs that are designed to prepare students for careers in professional and academic fields.\n",
      "---\n",
      "\n",
      "Q: What is EPEP?\n",
      "A: EPEP is an acronym that stands for \"Electroencephalographic Perceptual Evaluation of Picture Quality.\"\n",
      "---\n",
      "\n",
      "Q: What is BER?\n",
      "A: BER is a program and dataset for machine learning tasks, such as computer vision and natural language processing, that comes with pre-trained layers and architectures that have been pre-trained on a large dataset. It is a widely-used dataset that is commonly used in these tasks.\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Test questions\n",
    "questions = [\n",
    "    \"What is EEC?\",\n",
    "    \"Explain what is APU.\",\n",
    "    \"What can I see in MM03?\",\n",
    "    \"What is AHEAD?\",\n",
    "    \"What is EPEP?\",\n",
    "    \"What is BER?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    prompt = f\"### Prompt:\\n{question}\\n\\n### Response:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(**inputs, \n",
    "                             max_new_tokens=256, \n",
    "                             do_sample=True,\n",
    "                             temperature=0.7)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True).split(\"### Response:\")[-1].strip()\n",
    "    print(f\"\\nQ: {question}\")\n",
    "    print(f\"A: {response}\\n---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27400d9e",
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "sys.exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5090fe",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a5c443",
   "metadata": {},
   "source": [
    "### 3- Preparing for PEFT - Applying LoRA/QLoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e33878d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare for PEFT\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# LoRA Config\n",
    "lora_config = LoraConfig(\n",
    "    r=8, # Rank of the LoRA matrix, the number of trainable parameters (the higher the more trainable parameters, but also more memory and computation)\n",
    "    lora_alpha=32, # Scaling factor for the LoRA matrix\n",
    "    target_modules=[\"q_proj\", \"v_proj\"], # Query and Value projection layers for TinyLlama\n",
    "    lora_dropout=0.05, # Dropout rate for the LoRA matrix\n",
    "    bias=\"none\", # Bias for the LoRA matrix (not used for TinyLlama)\n",
    "    task_type=\"CAUSAL_LM\" # Task type for the LoRA matrix\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59db8d3f",
   "metadata": {},
   "source": [
    "### Trainable Parameters Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b2d32f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,126,400 || all params: 1,101,174,784 || trainable%: 0.1023\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.print_trainable_parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4661d1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a61c77",
   "metadata": {},
   "source": [
    "### 4- Data Collator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910ab558",
   "metadata": {},
   "source": [
    "A data collator is a crucial component in the training pipeline that prepares batches of data for the model. \n",
    "In this case, we're using DataCollatorForLanguageModeling which:\n",
    "1. Pads sequences to the same length within each batch\n",
    "2. Creates attention masks to handle the padding\n",
    "3. Prepares the labels for language modeling\n",
    "\n",
    "The mlm=False parameter indicates we're doing causal language modeling (predicting next token) \n",
    "rather than masked language modeling (predicting masked tokens).\n",
    "\n",
    "This collator is necessary because:\n",
    "- It ensures all sequences in a batch have the same length through padding\n",
    "- It properly formats the input for the model's forward pass\n",
    "- It handles the creation of labels for the language modeling task\n",
    "- It optimizes memory usage by padding only within each batch rather than to a fixed length -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "424711c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer, \n",
    "                                                mlm=False) # As we are working with Causal Language Modeling (predicting next token), not MLM (Masked Language Modeling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d0d6df",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916420f4",
   "metadata": {},
   "source": [
    "### 5- Configure Training Arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20af8e32",
   "metadata": {},
   "source": [
    "I'll  avoid Trainer, its returning error when trying to train():\n",
    "\n",
    "\"The safest path on your setup is to avoid Trainer and instead train using a custom training loop with Accelerate, which gives you more control and avoids hidden offloading. Sometimes Hugging Face's Trainer tries to offload to CPU automatically if it detects low VRAM.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61adce46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "# Training Arguments - Optimized for GTX 1050 4GB VRAM with TinyLlama 1.1B + LoRA\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_model_dir,\n",
    "    per_device_train_batch_size=1,  # Keep small batch size for 4GB VRAM\n",
    "    gradient_accumulation_steps=16,  # Maintain effective batch size\n",
    "    optim=\"adamw_8bit\",  # Switching from Adamw_32bit to 8-bit optimizer from BitsAndBytes\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    logging_steps=20,\n",
    "    num_train_epochs=3,\n",
    "    max_steps=60, # Change to 200 when ready to production. I'll keep smaller for developing purposes\n",
    "    bf16=False,  # Disable bf16 since we're using fp16\n",
    "    fp16=True,  # Use fp16 for training\n",
    "    torch_compile=False,  # Disable torch compilation\n",
    "    gradient_checkpointing=True,\n",
    "    warmup_steps=50,\n",
    "    max_grad_norm=0.3,\n",
    "    ddp_find_unused_parameters=False,\n",
    "    dataloader_num_workers=0,       # For pre-loading batches in background\n",
    "    remove_unused_columns=False,\n",
    "    group_by_length=True,  # Group similar length sequences for efficiency\n",
    "    save_strategy=\"epoch\", \n",
    "    save_total_limit=5, # Only 5 last checkpoints\n",
    "    # report_to=\"mlflow\"  # MLFlow reporting\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    peft_config=lora_config,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[\n",
    "        # MLflowCallback()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e716bc3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4955b5",
   "metadata": {},
   "source": [
    "### 6 - Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9e805ed7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='160' max='160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [160/160 18:33, Epoch 160/160]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.249100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.058300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.046400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.043100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.042700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.041900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.041200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.041700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=160, training_loss=0.07055153772234916, metrics={'train_runtime': 1122.2936, 'train_samples_per_second': 2.281, 'train_steps_per_second': 0.143, 'total_flos': 163050970152960.0, 'train_loss': 0.07055153772234916})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Monitoring with MLFlow    \n",
    "# with mlflow.start_run():\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62ffc25",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc1dde8",
   "metadata": {},
   "source": [
    "### 7- Merging with Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8c397ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the peft_model that is already loaded in the trainer\n",
    "peft_model = trainer.model  # AutoPeftModelForCausalLM with the weights of the last checkpoint\n",
    "\n",
    "# Merge the LoRA delta-weights into the base model and remove the PEFT wrapper\n",
    "merged_model = peft_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c834022e",
   "metadata": {},
   "source": [
    "### 8- Saving Fine-Tuned LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07e5839",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'DTensor' from 'torch.distributed.tensor' (c:\\Users\\Paulo\\Documents\\repos\\fine-tuning-llms\\venv\\Lib\\site-packages\\torch\\distributed\\tensor\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Saving merged model\u001b[39;00m\n\u001b[32m      2\u001b[39m os.makedirs(merged_model_dir, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mmerged_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmerged_model_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Saving tokenizer\u001b[39;00m\n\u001b[32m      6\u001b[39m tokenizer.save_pretrained(merged_model_dir)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Paulo\\Documents\\repos\\fine-tuning-llms\\venv\\Lib\\site-packages\\transformers\\modeling_utils.py:3572\u001b[39m, in \u001b[36mPreTrainedModel.save_pretrained\u001b[39m\u001b[34m(self, save_directory, is_main_process, state_dict, save_function, push_to_hub, max_shard_size, safe_serialization, variant, token, save_peft_format, **kwargs)\u001b[39m\n\u001b[32m   3568\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, tensor \u001b[38;5;129;01min\u001b[39;00m state_dict.items():\n\u001b[32m   3569\u001b[39m     \u001b[38;5;66;03m# Sometimes in the state_dict we have non-tensor objects.\u001b[39;00m\n\u001b[32m   3570\u001b[39m     \u001b[38;5;66;03m# e.g. in bitsandbytes we have some `str` objects in the state_dict\u001b[39;00m\n\u001b[32m   3571\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensor, torch.Tensor) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensor, DTensor):\n\u001b[32m-> \u001b[39m\u001b[32m3572\u001b[39m         ptrs[\u001b[43mid_tensor_storage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m)\u001b[49m].append(name)\n\u001b[32m   3573\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3574\u001b[39m         \u001b[38;5;66;03m# In the non-tensor case, fall back to the pointer of the object itself\u001b[39;00m\n\u001b[32m   3575\u001b[39m         ptrs[\u001b[38;5;28mid\u001b[39m(tensor)].append(name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Paulo\\Documents\\repos\\fine-tuning-llms\\venv\\Lib\\site-packages\\transformers\\pytorch_utils.py:300\u001b[39m, in \u001b[36mid_tensor_storage\u001b[39m\u001b[34m(tensor)\u001b[39m\n\u001b[32m    293\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    294\u001b[39m \u001b[33;03mUnique identifier to a tensor storage. Multiple different tensors can share the same underlying storage. For\u001b[39;00m\n\u001b[32m    295\u001b[39m \u001b[33;03mexample, \"meta\" tensors all share the same storage, and thus their identifier will all be equal. This identifier is\u001b[39;00m\n\u001b[32m    296\u001b[39m \u001b[33;03mguaranteed to be unique and constant for this tensor's storage during its lifetime. Two tensor storages with\u001b[39;00m\n\u001b[32m    297\u001b[39m \u001b[33;03mnon-overlapping lifetimes may have the same id.\u001b[39;00m\n\u001b[32m    298\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    299\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_torch_greater_or_equal_than_2_0:\n\u001b[32m--> \u001b[39m\u001b[32m300\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistributed\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtensor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DTensor\n\u001b[32m    302\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensor, DTensor):\n\u001b[32m    303\u001b[39m         local_tensor = tensor.to_local()\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'DTensor' from 'torch.distributed.tensor' (c:\\Users\\Paulo\\Documents\\repos\\fine-tuning-llms\\venv\\Lib\\site-packages\\torch\\distributed\\tensor\\__init__.py)"
     ]
    }
   ],
   "source": [
    "# Saving merged model\n",
    "os.makedirs(merged_model_dir, exist_ok=True)\n",
    "merged_model.save_pretrained(merged_model_dir, safe_serialization=True)\n",
    "\n",
    "# Saving tokenizer\n",
    "tokenizer.save_pretrained(merged_model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd180df0",
   "metadata": {},
   "source": [
    "### 9- Testing Fine-Tuned Model in Same Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4bb904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assert model is in GPU\n",
    "merged_model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
